---
---

@inproceedings{he2024videoscore,
    title = "VideoScore: Building Automatic Metrics to Simulate Fine-grained Human Feedback for Video Generation",
    author = {Xuan He and Dongfu Jiang and Ge Zhang and Max Ku and Achint Soni and Sherman Siu and Haonan Chen and Abhranil Chandra and Ziyan Jiang and Aaran Arulraj and Kai Wang and Quy Duc Do and Yuansheng Ni and Bohan Lyu and Yaswanth Narsupalli and Rongqi Fan and Zhiheng Lyu and Bill Yuchen Lin and Wenhu Chen},
    booktitle = "Proceedings of EMNLP",
    month = nov,
    year = "2024",
    arxiv = "2406.15252",
    address = "Miami, US",
    publisher = "Association for Computational Linguistics",
    url={https://openreview.net/forum?id=PcUvvKzULn},
    abstract = "The recent years have witnessed great advances in video generation. However, the development of automatic video metrics is lagging significantly behind. None of the existing metric is able to provide reliable scores over generated videos. The main barrier is the lack of large-scale human-annotated dataset. In this paper, we release VideoFeedback, the first large-scale dataset containing human-provided multi-aspect score over 37.6K synthesized videos from 11 existing video generative models. We train VideoScore (initialized from Mantis) based on VideoFeedback to enable automatic video quality assessment. Experiments show that the Spearman correlation between VideoScore and humans can reach 77.1 on VideoFeedback-test, beating the prior best metrics by about 50 points. Further result on other held-out EvalCrafter, GenAI-Bench, and VBench show that VideoScore has consistently much higher correlation with human judges than other metrics. Due to these results, we believe VideoScore can serve as a great proxy for human raters to (1) rate different video models to track progress (2) simulate fine-grained human feedback in Reinforcement Learning with Human Feedback (RLHF) to improve current video generation models.",
    preview={videoscore.png},
    website = "https://tiger-ai-lab.github.io/VideoScore/",
    github = "TIGER-AI-Lab/VideoScore",
    huggingface="https://huggingface.co/collections/TIGER-Lab/videoscore-6678c9451192e834e91cc0bf",
    twitter="https://twitter.com/DongfuJiang/status/1805438506137010326",
    selected = true,
    num_co_first_author = 2,
    abbr={EMNLP 2024},
    bibtex_show={true},
}

@inproceedings{Lu2024WildVisionEV,
  title={WildVision: Evaluating Vision-Language Models in the Wild with Human Preferences},
  author={Yujie Lu and Dongfu Jiang and Wenhu Chen and William Yang Wang and Yejin Choi and Bill Yuchen Lin},
  booktitle = "Proceedings of NeurIPS 2024 Datasets and Benchmarks Track",
  address = "Vancouver, Canada",
  month={Dec},
  year={2024},
  url={https://openreview.net/forum?id=i92eyFCQHC#discussion},
  abstract={Recent breakthroughs in vision-language models (VLMs) emphasize the necessity of benchmarking human preferences in real-world multimodal interactions. To address this gap, we launched WildVision-Arena (WV-Arena), an online platform that collects human preferences to evaluate VLMs. We curated WV-Bench by selecting 500 high-quality samples from 8,000 user submissions in WV-Arena. WV-Bench uses GPT-4 as the judge to compare each VLM with Claude-3-Sonnet, achieving a Spearman correlation of 0.94 with the WV-Arena Elo. This significantly outperforms other benchmarks like MMVet, MMMU, and MMStar.

  Our comprehensive analysis of 20K real-world interactions reveals important insights into the failure cases of top-performing VLMs. For example, we find that although GPT-4V surpasses many other models like Reka-Flash, Opus, and Yi-VL-Plus in simple visual recognition and reasoning tasks, it still faces challenges with subtle contextual cues, spatial reasoning, visual imagination, and expert domain knowledge. Additionally, current VLMs exhibit issues with hallucinations and safety when intentionally provoked. We are releasing our chat and feedback data to further advance research in the field of VLMs.},
  abbr={NeurIPS 2024},
  github={WildVision-AI/WildVision-Arena},
  preview={wildvision.png},
  arxiv={2406.11069},
  twitter = "https://twitter.com/billyuchenlin/status/1755207605537120513",
  huggingface="https://huggingface.co/spaces/WildVision/vision-arena",
  selected = true,
  bibtex_show={true},
}

@inproceedings{Jiang2024GenAIAA,
  title={GenAI Arena: An Open Evaluation Platform for Generative Models},
  author={Dongfu Jiang and Max Ku and Tianle Li and Yuansheng Ni and Shizhuo Sun and Rongqi Fan and Wenhu Chen},
  booktitle = "Proceedings of NeurIPS 2024 Datasets and Benchmarks Track",
  address = "Vancouver, Canada",
  month={Dec},
  year={2024},
  url={https://openreview.net/forum?id=0Gmi8TkUC7#discussion},
  abstract = {Generative AI has made remarkable strides to revolutionize fields such as image and video generation. These advancements are driven by innovative algorithms, architecture, and data. However, the rapid proliferation of generative models has highlighted a critical gap: the absence of trustworthy evaluation metrics. Current automatic assessments such as FID, CLIP, FVD, etc often fail to capture the nuanced quality and user satisfaction associated with generative outputs. This paper proposes an open platform \arena to evaluate different image and video generative models, where users can actively participate in evaluating these models. By leveraging collective user feedback and votes, \arena aims to provide a more democratic and accurate measure of model performance. It covers three arenas for text-to-image generation, text-to-video generation, and image editing respectively. Currently, we cover a total of 27 open-source generative models. \arena has been operating for four months, amassing over 6000 votes from the community. We describe our platform, analyze the data, and explain the statistical methods for ranking the models. To further promote the research in building model-based evaluation metrics, we release a cleaned version of our preference data for the three tasks, namely GenAI-Bench. We prompt the existing multi-modal models like Gemini, GPT-4o to mimic human voting. We compute the correlation between model voting with human voting to understand their judging abilities. Our results show existing multimodal models are still lagging in assessing the generated visual content, even the best model GPT-4o only achieves a Pearson correlation of 0.22 in quality subscore, and behave like random guessing in others.},
  abbr={NeurIPS 2024},
  github={TIGER-AI-Lab/GenAI-Arena},
  preview={genai-arena.png},
  arxiv={2406.04485},
  huggingface="https://huggingface.co/spaces/TIGER-Lab/GenAI-Arena",
  selected = true,
  num_co_first_author = {3},
  bibtex_show={true},
}

@article{jiang2024mantis,
  title={MANTIS: Interleaved Multi-Image Instruction Tuning}, 
  author={Dongfu Jiang and Xuan He and Huaye Zeng and Cong Wei and Max Ku and Qian Liu and Wenhu Chen},
  abstract={Large multimodal models (LMMs) have shown great results in single-image vision language tasks. However, their abilities to solve multi-image visual language tasks is yet to be improved. The existing LMMs like OpenFlamingo, Emu2, Idefics gain their multi-image ability through pre-training on hundreds of millions of noisy interleaved image-text data from the web, which is neither efficient nor effective. In this paper, we aim to build strong multi-image LMMs via instruction tuning with academic-level resources. Therefore, we meticulously construct Mantis-Instruct containing 721K multi-image instruction data to train a family of models Mantis. The instruction tuning empowers Mantis with different multi-image skills like co-reference, comparison, reasoning, and temporal understanding. We evaluate Mantis on five multi-image benchmarks and seven single-image benchmarks. Mantis-SigLIP can achieve SoTA results on all the multi-image benchmarks and beat the strongest multi-image baseline, Idefics2-8B by an average of 11 absolute points. Notably, Idefics2-8B was pre-trained on 140M interleaved multi-image data, which is 200x larger than Mantis-Instruct. We observe that Mantis performs equivalently well on the held-in and held-out benchmarks, which shows its generalization ability. Notably, we found that Mantis can even match the performance of GPT-4V on multi-image benchmarks. We further evaluate Mantis on single-image benchmarks and demonstrate that Mantis also maintains a strong single-image performance on par with CogVLM and Emu2. Our results show that multi-image abilities are not necessarily gained through massive pre-training, instead, it can be gained by the low-cost instruction tuning. Our work provides new perspectives on how to improve LMMs' multi-image abilities.},
  journal={ArXiv},
  booltitle={ArXiv},
  year={2024},
  eprint={2405.01483},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2405.01483},
  website = "https://tiger-ai-lab.github.io/Mantis/",
  github = "TIGER-AI-Lab/Mantis",
  abbr={arXiv},
  preview={mantis_preview.png},
  arxiv={2405.01483},
  twitter = "https://twitter.com/DongfuJiang/status/1786552974598078677",
  huggingface="https://huggingface.co/collections/TIGER-Lab/mantis-6619b0834594c878cdb1d6e4",
  selected = true,
  bibtex_show={true},
}

@inproceedings{Ku2023VIEScoreTE,
  title={VIEScore: Towards Explainable Metrics for Conditional Image Synthesis Evaluation},
  author={Max Ku and Dongfu Jiang and Cong Wei and Xiang Yue and Wenhu Chen},
  booktitle = "Proceedings of ACL",
  publisher = "Association for Computational Linguistics",
  month = aug,
  year={2024},
  volume={abs/2312.14867},
  url = "https://aclanthology.org/2024.acl-long.663",
  doi = "10.18653/v1/2024.acl-long.663",
  pages = "12268--12290",
  abstract = "In the rapidly advancing field of conditional image generation research, challenges such as limited explainability lie in effectively evaluating the performance and capabilities of various models. This paper introduces VIEScore, a Visual Instruction-guided Explainable metric for evaluating any conditional image generation tasks. VIEScore leverages general knowledge from Multimodal Large Language Models (MLLMs) as the backbone and does not require training or fine-tuning. We evaluate VIEScore on seven prominent tasks in conditional image tasks and found: (1) VIEScore (GPT4-o) achieves a high Spearman correlation of 0.4 with human evaluations, while the human-to-human correlation is 0.45. (2) VIEScore (with open-source MLLM) is significantly weaker than GPT-4o and GPT-4v in evaluating synthetic images. (3) VIEScore achieves a correlation on par with human ratings in the generation tasks but struggles in editing tasks. With these results, we believe VIEScore shows its great potential to replace human judges in evaluating image synthesis tasks.",
  website = "https://tiger-ai-lab.github.io/VIEScore/",
  github = "TIGER-AI-Lab/VIEScore",
  abbr={ACL 2024},
  preview={viescore.png},
  arxiv={2312.14867},
  twitter = "https://twitter.com/DongfuJiang/status/1742043191732302076",
  selected = false,
  bibtex_show={true},
}

@inproceedings{Yue2023MMMUAM,
  title={MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI},
  author={Xiang Yue and Yuansheng Ni and Kai Zhang and Tianyu Zheng and Ruoqi Liu and Ge Zhang and Samuel Stevens and Dongfu Jiang and Weiming Ren and Yuxuan Sun and Cong Wei and Botao Yu and Ruibin Yuan and Renliang Sun and Ming Yin and Boyuan Zheng and Zhenzhu Yang and Yibo Liu and Wenhao Huang and Huan Sun and Yu Su and Wenhu Chen},
  month     = {June},
  address   = {Seattle, US},
  abstract = {We introduce MMMU: a new benchmark designed to evaluate multimodal models on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning. MMMU includes 11.5K meticulously collected multimodal questions from college exams quizzes and textbooks covering six core disciplines: Art & Design Business Science Health & Medicine Humanities & Social Science and Tech & Engineering. These questions span 30 subjects and 183 subfields comprising 30 highly heterogeneous image types such as charts diagrams maps tables music sheets and chemical structures. Unlike existing benchmarks MMMU focuses on advanced perception and reasoning with domain-specific knowledge challenging models to perform tasks akin to those faced by experts. The evaluation of 28 open-source LMMs as well as the proprietary GPT-4V(ision) and Gemini highlights the substantial challenges posed by MMMU. Even the advanced GPT-4V and Gemini Ultra only achieve accuracies of 56% and 59% respectively indicating significant room for improvement. We believe MMMU will stimulate the community to build next-generation multimodal foundation models towards expert artificial general intelligence.},
  year={2023},
  booktitle = {Proceedings of CVPR <span style="color: red; font-weight: bold;">oral</span>},
  url={https://arxiv.org/abs/2311.16502},
  website = "https://mmmu-benchmark.github.io",
  github = "MMMU-Benchmark/MMMU",
  preview={mmmu_preview.png},
  arxiv={2311.16502},
  twitter="https://twitter.com/xiangyue96/status/1729698316554801358",
  huggingface="https://huggingface.co/datasets/MMMU/MMMU",
  selected = true,
  bibtex_show={true},
  abbr={CVPR 2024},
  num_co_first_author = 4,
}


@article{jiang2024tigerscore,
  title={{TIGERS}core: Towards Building Explainable Metric for All Text Generation Tasks},
  author={Dongfu Jiang and Yishan Li and Ge Zhang and Wenhao Huang and Bill Yuchen Lin and Wenhu Chen},
  journal={Transactions on Machine Learning Research (TMLR)},
  year={2024},
  month={May},
  bibtex_show={true},
  abstract = {We present TIGERScore, a \textbf{T}rained metric that follows \textbf{I}nstruction \textbf{G}uidance to perform \textbf{E}xplainable, and \textbf{R}eference-free evaluation over a wide spectrum of text generation tasks. 
  Different from other automatic evaluation methods that only provide arcane scores, TIGERScore is guided by natural language instruction to provide error analysis to pinpoint the mistakes in the generated text. Our metric is based on LLaMA-2, trained on our meticulously curated instruction-tuning dataset MetricInstruct which covers 6 text generation tasks and 23 text generation datasets. The dataset consists of 42K quadruple in the form of (instruction, input, system output 
   error analysis). We collected the `system outputs' through from a large variety of models to cover different types of errors.
  To quantitatively assess our metric, we evaluate its correlation with human ratings on 5 held-in datasets, 2 held-out datasets and show that \metricname{} can achieve the open-source SoTA correlation with human ratings across these datasets and almost approaches GPT-4 evaluator. As a reference-free metric, its correlation can even surpass the best existing reference-based metrics. To further qualitatively assess the rationale generated by our metric, we conduct human evaluation on the generated explanations and found that the explanations are 70.8\% accurate.
  Through these experimental results, we believe \metricname{} demonstrates the possibility of building universal explainable metrics to evaluate any text generation task.},
  selected = true,
  github = "TIGER-AI-Lab/TIGERScore",
  website = "https://tiger-ai-lab.github.io/TIGERScore/",
  arxiv={2310.00752},
  twitter = "https://twitter.com/DongfuJiang/status/1735508082510168425",
  huggingface="https://huggingface.co/collections/TIGER-Lab/tigerscore-657020bfae61260b6131f1ca",
  slides="https://docs.google.com/presentation/d/1-0uYzlR_-hUpgdfEksFWb65l0UouJ0W_/edit?usp=sharing&ouid=112776303426789186559&rtpof=true&sd=true",
  abbr={TMLR 2024},
  issn={2835-8856},
  url={https://openreview.net/forum?id=EE1CBKC0SZ},
  preview={tigerscore_preview.png},
  num_co_first_author = 2,
}

@inproceedings{jiang-etal-2023-llm,
  title = "{LLM}-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion",
  author = "Jiang, Dongfu  and
    Ren, Xiang  and
    Lin, Bill Yuchen",
  booktitle = "Proceedings of ACL",
  month = jul,
  year = "2023",
  address = "Toronto, Canada",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2023.acl-long.792",
  doi = "10.18653/v1/2023.acl-long.792",
  pages = "14165--14178",
  abstract = "We present LLM-Blender, an ensembling framework designed to attain consistently superior performance by leveraging the diverse strengths of multiple open-source large language models (LLMs). Our framework consists of two modules: PairRanker and GenFuser, addressing the observation that optimal LLMs for different examples can significantly vary. PairRanker employs a specialized pairwise comparison method to distinguish subtle differences between candidate outputs. It jointly encodes the input text and a pair of candidates, using cross-attention encoders to determine the superior one. Our results demonstrate that PairRanker exhibits the highest correlation with ChatGPT-based ranking. Then, GenFuser aims to merge the top-ranked candidates, generating an improved output by capitalizing on their strengths and mitigating their weaknesses. To facilitate large-scale evaluation, we introduce a benchmark dataset, MixInstruct, which is a mixture of multiple instruction datasets featuring oracle pairwise comparisons. Our LLM-Blender significantly outperform individual LLMs and baseline methods across various metrics, establishing a substantial performance gap.",
  selected = true,
  website = "https://yuchenlin.xyz/LLM-Blender/",
  github = "yuchenlin/LLM-Blender",
  blog = "https://blog.allenai.org/llm-blender-a-simple-ensemble-learning-framework-for-llms-9e4bc57af23e",
  twitter = "https://twitter.com/billyuchenlin/status/1668666357058277377",
  huggingface="https://huggingface.co/llm-blender",
  slides="https://docs.google.com/presentation/d/1UvTYEjIjpPGjrvBxtEmAtC06N-7xFFY9/edit?usp=sharing&ouid=112776303426789186559&rtpof=true&sd=true",
  bibtex_show={true},
  abbr={ACL 2023},
  preview={llm_blender_preview.png},
  arxiv={2306.02561},
}