---
---

@article{he2024videoscore,
  title={VideoScore: Building Automatic Metrics to Simulate Fine-grained Human Feedback for Video Generation},
  author={Xuan He and Dongfu Jiang and Ge Zhang and Max Ku and Achint Soni and Sherman Siu and Haonan Chen and Abhranil Chandra and Ziyan Jiang and Aaran Arulraj and Kai Wang and Quy Duc Do and Yuansheng Ni and Bohan Lyu and Yaswanth Narsupalli and Rongqi Fan and Zhiheng Lyu and Yuchen Lin and Wenhu Chen},
  journal={ArXiv},
  booltitle={ArXiv},
  year={2024},
  eprint={2406.15252},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2406.15252},
  abbr={EMNLP 2024},
  preview={videoscore.png},
  arxiv={2406.15252},
  website = "https://tiger-ai-lab.github.io/VideoScore/",
  github = "TIGER-AI-Lab/VideoScore",
  huggingface="https://huggingface.co/collections/TIGER-Lab/videoscore-6678c9451192e834e91cc0bf",
  twitter="https://twitter.com/DongfuJiang/status/1805438506137010326",
  selected = true,
  num_co_first_author = 2,
  bibtex_show={true},
}

@article{Lu2024WildVisionEV,
  title={WildVision: Evaluating Vision-Language Models in the Wild with Human Preferences},
  author={Yujie Lu and Dongfu Jiang and Wenhu Chen and William Yang Wang and Yejin Choi and Bill Yuchen Lin},
  journal={ArXiv},
  booltitle={ArXiv},
  year={2024},
  eprint={2406.11069},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2406.11069},
  abbr={NeurIPS 2024 (D/B Track)},
  preview={wildvision.png},
  arxiv={2406.11069},
  twitter = "https://twitter.com/billyuchenlin/status/1755207605537120513",
  huggingface="https://huggingface.co/spaces/WildVision/vision-arena",
  selected = true,
  bibtex_show={true},
}


@article{Jiang2024GenAIAA,
  title={GenAI Arena: An Open Evaluation Platform for Generative Models},
  author={Dongfu Jiang and Max W.F. Ku and Tianle Li and Yuansheng Ni and Shizhuo Sun and Rongqi Fan and Wenhu Chen},
  journal={ArXiv},
  booltitle={ArXiv},
  year={2024},
  eprint={2406.04485},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2406.04485},
  github = "TIGER-AI-Lab/GenAI-Arena",
  abbr={NeurIPS 2024 (D/B Track)},
  preview={genai-arena.png},
  arxiv={2406.04485},
  huggingface="https://huggingface.co/spaces/TIGER-Lab/GenAI-Arena",
  selected = true,
  bibtex_show={true},
  num_co_first_author = 3,
}

@article{jiang2024mantis,
  title={MANTIS: Interleaved Multi-Image Instruction Tuning}, 
  author={Dongfu Jiang and Xuan He and Huaye Zeng and Cong Wei and Max Ku and Qian Liu and Wenhu Chen},
  journal={ArXiv},
  booltitle={ArXiv},
  year={2024},
  eprint={2405.01483},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2405.01483},
  website = "https://tiger-ai-lab.github.io/Mantis/",
  github = "TIGER-AI-Lab/Mantis",
  abbr={arXiv},
  preview={mantis_preview.png},
  arxiv={2405.01483},
  twitter = "https://twitter.com/DongfuJiang/status/1786552974598078677",
  huggingface="https://huggingface.co/collections/TIGER-Lab/mantis-6619b0834594c878cdb1d6e4",
  selected = true,
  bibtex_show={true},
}

@inproceedings{Ku2023VIEScoreTE,
  title={VIEScore: Towards Explainable Metrics for Conditional Image Synthesis Evaluation},
  author={Max W.F. Ku and Dongfu Jiang and Cong Wei and Xiang Yue and Wenhu Chen},
  booktitle={ACL},
  year={2024},
  volume={abs/2312.14867},
  url={https://arxiv.org/abs/2312.14867},
  website = "https://tiger-ai-lab.github.io/VIEScore/",
  github = "TIGER-AI-Lab/VIEScore",
  abbr={ACL 2024},
  preview={viescore.png},
  arxiv={2312.14867},
  twitter = "https://twitter.com/DongfuJiang/status/1742043191732302076",
  selected = true,
  bibtex_show={true},
}

@inproceedings{Yue2023MMMUAM,
  title={MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI},
  author={Xiang Yue and Yuansheng Ni and Kai Zhang and Tianyu Zheng and Ruoqi Liu and Ge Zhang and Samuel Stevens and Dongfu Jiang and Weiming Ren and Yuxuan Sun and Cong Wei and Botao Yu and Ruibin Yuan and Renliang Sun and Ming Yin and Boyuan Zheng and Zhenzhu Yang and Yibo Liu and Wenhao Huang and Huan Sun and Yu Su and Wenhu Chen},
  year={2023},
  booktitle = {CVPR <span style="color: red; font-weight: bold;">oral</span>},
  url={https://arxiv.org/abs/2311.16502},
  website = "https://mmmu-benchmark.github.io",
  github = "MMMU-Benchmark/MMMU",
  preview={mmmu_preview.png},
  arxiv={2311.16502},
  twitter="https://twitter.com/xiangyue96/status/1729698316554801358",
  huggingface="https://huggingface.co/datasets/MMMU/MMMU",
  selected = true,
  bibtex_show={true},
  abbr={CVPR 2024},
  num_co_first_author = 4,
}

@article{jiang2024tigerscore,
  title={{TIGERS}core: Towards Building Explainable Metric for All Text Generation Tasks},
  author={Dongfu Jiang and Yishan Li and Ge Zhang and Wenhao Huang and Bill Yuchen Lin and Wenhu Chen},
  journal={Transactions on Machine Learning Research},
  year={2024},
  bibtex_show={true},
  abstract = {We present TIGERScore, a \textbf{T}rained metric that follows \textbf{I}nstruction \textbf{G}uidance to perform \textbf{E}xplainable, and \textbf{R}eference-free evaluation over a wide spectrum of text generation tasks. 
  Different from other automatic evaluation methods that only provide arcane scores, TIGERScore is guided by natural language instruction to provide error analysis to pinpoint the mistakes in the generated text. Our metric is based on LLaMA-2, trained on our meticulously curated instruction-tuning dataset MetricInstruct which covers 6 text generation tasks and 23 text generation datasets. The dataset consists of 42K quadruple in the form of (instruction, input, system output 
   error analysis). We collected the `system outputs' through from a large variety of models to cover different types of errors.
  To quantitatively assess our metric, we evaluate its correlation with human ratings on 5 held-in datasets, 2 held-out datasets and show that \metricname{} can achieve the open-source SoTA correlation with human ratings across these datasets and almost approaches GPT-4 evaluator. As a reference-free metric, its correlation can even surpass the best existing reference-based metrics. To further qualitatively assess the rationale generated by our metric, we conduct human evaluation on the generated explanations and found that the explanations are 70.8\% accurate.
  Through these experimental results, we believe \metricname{} demonstrates the possibility of building universal explainable metrics to evaluate any text generation task.},
  selected = true,
  github = "TIGER-AI-Lab/TIGERScore",
  website = "https://tiger-ai-lab.github.io/TIGERScore/",
  arxiv={2310.00752},
  twitter = "https://twitter.com/DongfuJiang/status/1735508082510168425",
  huggingface="https://huggingface.co/collections/TIGER-Lab/tigerscore-657020bfae61260b6131f1ca",
  slides="https://docs.google.com/presentation/d/1-0uYzlR_-hUpgdfEksFWb65l0UouJ0W_/edit?usp=sharing&ouid=112776303426789186559&rtpof=true&sd=true",
  abbr={TMLR 2024},
  issn={2835-8856},
  url={https://openreview.net/forum?id=EE1CBKC0SZ},
  preview={tigerscore_preview.png},
  num_co_first_author = 2,
}

@inproceedings{jiang-etal-2023-llm,
  title = "{LLM}-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion",
  author = "Jiang, Dongfu  and
    Ren, Xiang  and
    Lin, Bill Yuchen",
  booktitle = "ACL",
  month = jul,
  year = "2023",
  address = "Toronto, Canada",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2023.acl-long.792",
  doi = "10.18653/v1/2023.acl-long.792",
  pages = "14165--14178",
  abstract = "We present LLM-Blender, an ensembling framework designed to attain consistently superior performance by leveraging the diverse strengths of multiple open-source large language models (LLMs). Our framework consists of two modules: PairRanker and GenFuser, addressing the observation that optimal LLMs for different examples can significantly vary. PairRanker employs a specialized pairwise comparison method to distinguish subtle differences between candidate outputs. It jointly encodes the input text and a pair of candidates, using cross-attention encoders to determine the superior one. Our results demonstrate that PairRanker exhibits the highest correlation with ChatGPT-based ranking. Then, GenFuser aims to merge the top-ranked candidates, generating an improved output by capitalizing on their strengths and mitigating their weaknesses. To facilitate large-scale evaluation, we introduce a benchmark dataset, MixInstruct, which is a mixture of multiple instruction datasets featuring oracle pairwise comparisons. Our LLM-Blender significantly outperform individual LLMs and baseline methods across various metrics, establishing a substantial performance gap.",
  selected = true,
  website = "https://yuchenlin.xyz/LLM-Blender/",
  github = "yuchenlin/LLM-Blender",
  blog = "https://blog.allenai.org/llm-blender-a-simple-ensemble-learning-framework-for-llms-9e4bc57af23e",
  twitter = "https://twitter.com/billyuchenlin/status/1668666357058277377",
  huggingface="https://huggingface.co/llm-blender",
  slides="https://docs.google.com/presentation/d/1UvTYEjIjpPGjrvBxtEmAtC06N-7xFFY9/edit?usp=sharing&ouid=112776303426789186559&rtpof=true&sd=true",
  bibtex_show={true},
  abbr={ACL 2023},
  preview={llm_blender_preview.png},
  arxiv={2306.02561},
}

