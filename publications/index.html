<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Publications | Dongfu Jiang (姜东甫)</title> <meta name="author" content="Dongfu Jiang (姜东甫)"> <meta name="description" content="publications by categories in reversed chronological order. generated by jekyll-scholar."> <meta name="keywords" content="natural language processing, large language models, Uwaterloo"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/avatar_square.jpg?06012969406defbc5265053addcc1c8e"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jdf-prog.github.io/publications/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Dongfu </span>Jiang (姜东甫)</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/Misc/">Miscellaneous</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description">publications by categories in reversed chronological order. generated by jekyll-scholar.</p> </header> <article> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Arxiv</abbr></div> <div id="Zeng2025ACECODERAC" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2502.01718" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank">ACECODER: Acing Coder RL via Automated Test-Case Synthesis</a></div> <div class="author"> Huaye Zeng<sup>*</sup>, <em><b>Dongfu Jiang</b></em><sup>*</sup>, Haozhe Wang, Ping Nie, Xiaotong Chen, and Wenhu Chen</div> <div class="periodical"> <em>In arxiv preprint</em>, Feb 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2502.01718" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/badge/arXiv-b31b1b.svg?logo=arXiv"> </a> <a class="abstract z-depth-0" role="button"><img alt="Abstract" src="https://img.shields.io/badge/-Abs-FFCC99?style=flat"></a> <a class="bibtex z-depth-0" role="button"><img alt="Bibtex" src="https://img.shields.io/badge/-Bib-FFCC99?style=flat"></a> <a href="https://tiger-ai-lab.github.io/AceCoder/" rel="external nofollow noopener" target="_blank"> <img alt="Website" src="https://img.shields.io/badge/-Website-FFCC99?style=flat"> </a> <a href="https://huggingface.co/collections/TIGER-Lab/acecoder-67a16011a6c7d65cad529eba" rel="external nofollow noopener" target="_blank"> <img alt="Huggingface Collections" src="https://img.shields.io/badge/-%F0%9F%A4%97%20Huggingface-red?style=flat"> </a> <a href="https://x.com/DongfuJiang/status/1886828310841204859" rel="external nofollow noopener" target="_blank"><img alt="X (formerly Twitter) URL" src="https://img.shields.io/badge/-Tweet-blue?style=flat&amp;logo=twitter"></a> <a href="https://github.com/TIGER-AI-Lab/AceCoder" rel="external nofollow noopener" target="_blank"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/TIGER-AI-Lab/AceCoder"></a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Most progress in recent coder models has been driven by supervised fine-tuning (SFT), while the potential of reinforcement learning (RL) remains largely unexplored, primarily due to the lack of reliable reward data/model in the code domain. In this paper, we address this challenge by leveraging automated large-scale test-case synthesis to enhance code model training. Specifically, we design a pipeline that generates extensive (question, test-cases) pairs from existing code data. Using these test cases, we construct preference pairs based on pass rates over sampled programs to train reward models with Bradley-Terry loss. It shows an average of 10-point improvement for Llama-3.1-8B-Ins and 5-point improvement for Qwen2.5-Coder-7B-Ins through best-of-32 sampling, making the 7B model on par with 236B DeepSeek-V2.5. Furthermore, we conduct reinforcement learning with both reward models and test-case pass rewards, leading to consistent improvements across HumanEval, MBPP, BigCodeBench, and LiveCodeBench (V4). Notably, we follow the R1-style training to start from Qwen2.5-Coder-base directly and show that our RL training can improve model on HumanEval-plus by over 25% and MBPP-plus by 6% for merely 80 optimization steps. We believe our results highlight the huge potential of reinforcement learning in coder models.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Zeng2025ACECODERAC</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ACECODER: Acing Coder RL via Automated Test-Case Synthesis}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zeng, Huaye and Jiang, Dongfu and Wang, Haozhe and Nie, Ping and Chen, Xiaotong and Chen, Wenhu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{arxiv preprint}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">feb</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">github</span> <span class="p">=</span> <span class="s">{TIGER-AI-Lab/AceCoder}</span><span class="p">,</span>
  <span class="na">huggingface</span> <span class="p">=</span> <span class="s">{https://huggingface.co/collections/TIGER-Lab/acecoder-67a16011a6c7d65cad529eba}</span><span class="p">,</span>
  <span class="na">twitter</span> <span class="p">=</span> <span class="s">{https://x.com/DongfuJiang/status/1886828310841204859}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">num_co_first_author</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/acecoder-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/acecoder-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/acecoder-1400.webp"></source> <img src="/assets/img/publication_preview/acecoder.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="acecoder.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="https://iclr.cc/" rel="external nofollow noopener" target="_blank">ICLR 2025</a></abbr></div> <div id="Chen2024MEGABenchSM" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2410.10563" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank">MEGA-Bench: Scaling Multimodal Evaluation to over 500 Real-World Tasks</a></div> <div class="author"> Jiacheng Chen<sup>*</sup>, Tianhao Liang<sup>*</sup>, Sherman Siu<sup>*</sup>, Zhengqing Wang, Kai Wang, Yubo Wang, Yuansheng Ni, Ziyan Jiang, and <span class="more-authors" title="click to view 8 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '8 more authors' ? 'Wang Zhu, Bohan Lyu, Dongfu Jiang, Xuan He, Yuan Liu, Hexiang Hu, Xiang Yue, Wenhu Chen' : '8 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">8 more authors</span> </div> <div class="periodical"> <em>In The Thirteenth International Conference on Learning Representations</em>, Feb 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2410.10563" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/badge/arXiv-b31b1b.svg?logo=arXiv"> </a> <a class="abstract z-depth-0" role="button"><img alt="Abstract" src="https://img.shields.io/badge/-Abs-FFCC99?style=flat"></a> <a class="bibtex z-depth-0" role="button"><img alt="Bibtex" src="https://img.shields.io/badge/-Bib-FFCC99?style=flat"></a> <a href="https://tiger-ai-lab.github.io/MEGA-Bench/" rel="external nofollow noopener" target="_blank"> <img alt="Website" src="https://img.shields.io/badge/-Website-FFCC99?style=flat"> </a> <a href="https://huggingface.co/spaces/TIGER-Lab/MEGA-Bench" rel="external nofollow noopener" target="_blank"> <img alt="Huggingface Collections" src="https://img.shields.io/badge/-%F0%9F%A4%97%20Huggingface-red?style=flat"> </a> <a href="https://x.com/WenhuChen/status/1846692920117678384" rel="external nofollow noopener" target="_blank"><img alt="X (formerly Twitter) URL" src="https://img.shields.io/badge/-Tweet-blue?style=flat&amp;logo=twitter"></a> <a href="https://github.com/TIGER-AI-Lab/MEGA-Bench" rel="external nofollow noopener" target="_blank"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/TIGER-AI-Lab/MEGA-Bench"></a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>We present MEGA-Bench, an evaluation suite that scales multimodal evaluation to over 500 real-world tasks, to address the highly heterogeneous daily use cases of end users. Our objective is to optimize for a set of high-quality data samples that cover a highly diverse and rich set of multimodal tasks, while enabling cost-effective and accurate model evaluation. In particular, we collected 505 realistic tasks encompassing over 8,000 samples from 16 expert annotators to extensively cover the multimodal task space. Instead of unifying these problems into standard multi-choice questions (like MMMU, MMBench, and MMT-Bench), we embrace a wide range of output formats like numbers, phrases, code, \LaTeX, coordinates, JSON, free-form, etc. To accommodate these formats, we developed over 40 metrics to evaluate these tasks. Unlike existing benchmarks, MEGA-Bench offers a fine-grained capability report across multiple dimensions (e.g., application, input type, output format, skill), allowing users to interact with and visualize model capabilities in depth. We evaluate a wide variety of frontier vision-language models on MEGA-Bench to understand their capabilities across these dimensions.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Chen2024MEGABenchSM</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{MEGA}-Bench: Scaling Multimodal Evaluation to over 500 Real-World Tasks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chen, Jiacheng and Liang, Tianhao and Siu, Sherman and Wang, Zhengqing and Wang, Kai and Wang, Yubo and Ni, Yuansheng and Jiang, Ziyan and Zhu, Wang and Lyu, Bohan and Jiang, Dongfu and He, Xuan and Liu, Yuan and Hu, Hexiang and Yue, Xiang and Chen, Wenhu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The Thirteenth International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=2rWbKbmOuM}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Singapore EXPO}</span><span class="p">,</span>
  <span class="na">github</span> <span class="p">=</span> <span class="s">{TIGER-AI-Lab/MEGA-Bench}</span><span class="p">,</span>
  <span class="na">huggingface</span> <span class="p">=</span> <span class="s">{https://huggingface.co/spaces/TIGER-Lab/MEGA-Bench}</span><span class="p">,</span>
  <span class="na">twitter</span> <span class="p">=</span> <span class="s">{https://x.com/WenhuChen/status/1846692920117678384}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">false</span><span class="p">,</span>
  <span class="na">num_co_first_author</span> <span class="p">=</span> <span class="s">{3}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/megabench_preview-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/megabench_preview-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/megabench_preview-1400.webp"></source> <img src="/assets/img/publication_preview/megabench_preview.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="megabench_preview.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#dc143c"><a href="https://2024.emnlp.org/" rel="external nofollow noopener" target="_blank">EMNLP 2024</a></abbr></div> <div id="he2024videoscore" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2406.15252" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank">VideoScore: Building Automatic Metrics to Simulate Fine-grained Human Feedback for Video Generation</a></div> <div class="author"> Xuan He<sup>*</sup>, <em><b>Dongfu Jiang</b></em><sup>*</sup>, Ge Zhang, Max Ku, Achint Soni, Sherman Siu, Haonan Chen, Abhranil Chandra, and <span class="more-authors" title="click to view 11 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '11 more authors' ? 'Ziyan Jiang, Aaran Arulraj, Kai Wang, Quy Duc Do, Yuansheng Ni, Bohan Lyu, Yaswanth Narsupalli, Rongqi Fan, Zhiheng Lyu, Bill Yuchen Lin, Wenhu Chen' : '11 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">11 more authors</span> </div> <div class="periodical"> <em>In Proceedings of EMNLP</em>, Nov 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2406.15252" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/badge/arXiv-b31b1b.svg?logo=arXiv"> </a> <a class="abstract z-depth-0" role="button"><img alt="Abstract" src="https://img.shields.io/badge/-Abs-FFCC99?style=flat"></a> <a class="bibtex z-depth-0" role="button"><img alt="Bibtex" src="https://img.shields.io/badge/-Bib-FFCC99?style=flat"></a> <a href="https://tiger-ai-lab.github.io/VideoScore/" rel="external nofollow noopener" target="_blank"> <img alt="Website" src="https://img.shields.io/badge/-Website-FFCC99?style=flat"> </a> <a href="https://huggingface.co/collections/TIGER-Lab/videoscore-6678c9451192e834e91cc0bf" rel="external nofollow noopener" target="_blank"> <img alt="Huggingface Collections" src="https://img.shields.io/badge/-%F0%9F%A4%97%20Huggingface-red?style=flat"> </a> <a href="https://twitter.com/DongfuJiang/status/1805438506137010326" rel="external nofollow noopener" target="_blank"><img alt="X (formerly Twitter) URL" src="https://img.shields.io/badge/-Tweet-blue?style=flat&amp;logo=twitter"></a> <a href="https://github.com/TIGER-AI-Lab/VideoScore" rel="external nofollow noopener" target="_blank"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/TIGER-AI-Lab/VideoScore"></a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>The recent years have witnessed great advances in video generation. However, the development of automatic video metrics is lagging significantly behind. None of the existing metric is able to provide reliable scores over generated videos. The main barrier is the lack of large-scale human-annotated dataset. In this paper, we release VideoFeedback, the first large-scale dataset containing human-provided multi-aspect score over 37.6K synthesized videos from 11 existing video generative models. We train VideoScore (initialized from Mantis) based on VideoFeedback to enable automatic video quality assessment. Experiments show that the Spearman correlation between VideoScore and humans can reach 77.1 on VideoFeedback-test, beating the prior best metrics by about 50 points. Further result on other held-out EvalCrafter, GenAI-Bench, and VBench show that VideoScore has consistently much higher correlation with human judges than other metrics. Due to these results, we believe VideoScore can serve as a great proxy for human raters to (1) rate different video models to track progress (2) simulate fine-grained human feedback in Reinforcement Learning with Human Feedback (RLHF) to improve current video generation models.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">he2024videoscore</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{VideoScore: Building Automatic Metrics to Simulate Fine-grained Human Feedback for Video Generation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{He, Xuan and Jiang, Dongfu and Zhang, Ge and Ku, Max and Soni, Achint and Siu, Sherman and Chen, Haonan and Chandra, Abhranil and Jiang, Ziyan and Arulraj, Aaran and Wang, Kai and Do, Quy Duc and Ni, Yuansheng and Lyu, Bohan and Narsupalli, Yaswanth and Fan, Rongqi and Lyu, Zhiheng and Lin, Bill Yuchen and Chen, Wenhu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of EMNLP}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Miami, US}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=PcUvvKzULn}</span><span class="p">,</span>
  <span class="na">github</span> <span class="p">=</span> <span class="s">{TIGER-AI-Lab/VideoScore}</span><span class="p">,</span>
  <span class="na">huggingface</span> <span class="p">=</span> <span class="s">{https://huggingface.co/collections/TIGER-Lab/videoscore-6678c9451192e834e91cc0bf}</span><span class="p">,</span>
  <span class="na">twitter</span> <span class="p">=</span> <span class="s">{https://twitter.com/DongfuJiang/status/1805438506137010326}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">num_co_first_author</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/videoscore-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/videoscore-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/videoscore-1400.webp"></source> <img src="/assets/img/publication_preview/videoscore.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="videoscore.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="https://neurips.cc/" rel="external nofollow noopener" target="_blank">NeurIPS 2024</a></abbr></div> <div id="Lu2024WildVisionEV" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2406.11069" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank">WildVision: Evaluating Vision-Language Models in the Wild with Human Preferences</a></div> <div class="author"> Yujie Lu, <em><b>Dongfu Jiang</b></em>, Wenhu Chen, William Yang Wang, Yejin Choi, and Bill Yuchen Lin</div> <div class="periodical"> <em>In Proceedings of NeurIPS 2024 Datasets and Benchmarks Track</em>, Dec 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2406.11069" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/badge/arXiv-b31b1b.svg?logo=arXiv"> </a> <a class="abstract z-depth-0" role="button"><img alt="Abstract" src="https://img.shields.io/badge/-Abs-FFCC99?style=flat"></a> <a class="bibtex z-depth-0" role="button"><img alt="Bibtex" src="https://img.shields.io/badge/-Bib-FFCC99?style=flat"></a> <a href="https://huggingface.co/spaces/WildVision/vision-arena" rel="external nofollow noopener" target="_blank"> <img alt="Huggingface Collections" src="https://img.shields.io/badge/-%F0%9F%A4%97%20Huggingface-red?style=flat"> </a> <a href="https://twitter.com/billyuchenlin/status/1755207605537120513" rel="external nofollow noopener" target="_blank"><img alt="X (formerly Twitter) URL" src="https://img.shields.io/badge/-Tweet-blue?style=flat&amp;logo=twitter"></a> <a href="https://github.com/WildVision-AI/WildVision-Arena" rel="external nofollow noopener" target="_blank"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/WildVision-AI/WildVision-Arena"></a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Recent breakthroughs in vision-language models (VLMs) emphasize the necessity of benchmarking human preferences in real-world multimodal interactions. To address this gap, we launched WildVision-Arena (WV-Arena), an online platform that collects human preferences to evaluate VLMs. We curated WV-Bench by selecting 500 high-quality samples from 8,000 user submissions in WV-Arena. WV-Bench uses GPT-4 as the judge to compare each VLM with Claude-3-Sonnet, achieving a Spearman correlation of 0.94 with the WV-Arena Elo. This significantly outperforms other benchmarks like MMVet, MMMU, and MMStar. Our comprehensive analysis of 20K real-world interactions reveals important insights into the failure cases of top-performing VLMs. For example, we find that although GPT-4V surpasses many other models like Reka-Flash, Opus, and Yi-VL-Plus in simple visual recognition and reasoning tasks, it still faces challenges with subtle contextual cues, spatial reasoning, visual imagination, and expert domain knowledge. Additionally, current VLMs exhibit issues with hallucinations and safety when intentionally provoked. We are releasing our chat and feedback data to further advance research in the field of VLMs.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Lu2024WildVisionEV</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{WildVision: Evaluating Vision-Language Models in the Wild with Human Preferences}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lu, Yujie and Jiang, Dongfu and Chen, Wenhu and Wang, William Yang and Choi, Yejin and Lin, Bill Yuchen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of NeurIPS 2024 Datasets and Benchmarks Track}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Vancouver, Canada}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=i92eyFCQHC#discussion}</span><span class="p">,</span>
  <span class="na">github</span> <span class="p">=</span> <span class="s">{WildVision-AI/WildVision-Arena}</span><span class="p">,</span>
  <span class="na">twitter</span> <span class="p">=</span> <span class="s">{https://twitter.com/billyuchenlin/status/1755207605537120513}</span><span class="p">,</span>
  <span class="na">huggingface</span> <span class="p">=</span> <span class="s">{https://huggingface.co/spaces/WildVision/vision-arena}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/wildvision-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/wildvision-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/wildvision-1400.webp"></source> <img src="/assets/img/publication_preview/wildvision.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="wildvision.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="https://neurips.cc/" rel="external nofollow noopener" target="_blank">NeurIPS 2024</a></abbr></div> <div id="Jiang2024GenAIAA" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2406.04485" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank">GenAI Arena: An Open Evaluation Platform for Generative Models</a></div> <div class="author"> <em><b>Dongfu Jiang</b></em><sup>*</sup>, Max Ku<sup>*</sup>, Tianle Li<sup>*</sup>, Yuansheng Ni, Shizhuo Sun, Rongqi Fan, and Wenhu Chen</div> <div class="periodical"> <em>In Proceedings of NeurIPS 2024 Datasets and Benchmarks Track</em>, Dec 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2406.04485" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/badge/arXiv-b31b1b.svg?logo=arXiv"> </a> <a class="abstract z-depth-0" role="button"><img alt="Abstract" src="https://img.shields.io/badge/-Abs-FFCC99?style=flat"></a> <a class="bibtex z-depth-0" role="button"><img alt="Bibtex" src="https://img.shields.io/badge/-Bib-FFCC99?style=flat"></a> <a href="https://huggingface.co/spaces/TIGER-Lab/GenAI-Arena" rel="external nofollow noopener" target="_blank"> <img alt="Huggingface Collections" src="https://img.shields.io/badge/-%F0%9F%A4%97%20Huggingface-red?style=flat"> </a> <a href="https://github.com/TIGER-AI-Lab/GenAI-Arena" rel="external nofollow noopener" target="_blank"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/TIGER-AI-Lab/GenAI-Arena"></a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Generative AI has made remarkable strides to revolutionize fields such as image and video generation. These advancements are driven by innovative algorithms, architecture, and data. However, the rapid proliferation of generative models has highlighted a critical gap: the absence of trustworthy evaluation metrics. Current automatic assessments such as FID, CLIP, FVD, etc often fail to capture the nuanced quality and user satisfaction associated with generative outputs. This paper proposes an open platform \arena to evaluate different image and video generative models, where users can actively participate in evaluating these models. By leveraging collective user feedback and votes, \arena aims to provide a more democratic and accurate measure of model performance. It covers three arenas for text-to-image generation, text-to-video generation, and image editing respectively. Currently, we cover a total of 27 open-source generative models. \arena has been operating for four months, amassing over 6000 votes from the community. We describe our platform, analyze the data, and explain the statistical methods for ranking the models. To further promote the research in building model-based evaluation metrics, we release a cleaned version of our preference data for the three tasks, namely GenAI-Bench. We prompt the existing multi-modal models like Gemini, GPT-4o to mimic human voting. We compute the correlation between model voting with human voting to understand their judging abilities. Our results show existing multimodal models are still lagging in assessing the generated visual content, even the best model GPT-4o only achieves a Pearson correlation of 0.22 in quality subscore, and behave like random guessing in others.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Jiang2024GenAIAA</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{GenAI Arena: An Open Evaluation Platform for Generative Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jiang, Dongfu and Ku, Max and Li, Tianle and Ni, Yuansheng and Sun, Shizhuo and Fan, Rongqi and Chen, Wenhu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of NeurIPS 2024 Datasets and Benchmarks Track}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Vancouver, Canada}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=0Gmi8TkUC7#discussion}</span><span class="p">,</span>
  <span class="na">github</span> <span class="p">=</span> <span class="s">{TIGER-AI-Lab/GenAI-Arena}</span><span class="p">,</span>
  <span class="na">huggingface</span> <span class="p">=</span> <span class="s">{https://huggingface.co/spaces/TIGER-Lab/GenAI-Arena}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">num_co_first_author</span> <span class="p">=</span> <span class="s">{3}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/genai-arena-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/genai-arena-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/genai-arena-1400.webp"></source> <img src="/assets/img/publication_preview/genai-arena.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="genai-arena.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#5b9bd5"><a href="https://jmlr.org/tmlr/" rel="external nofollow noopener" target="_blank">TMLR 2024</a></abbr></div> <div id="Jiang2024MANTISIM" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2405.01483" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank">MANTIS: Interleaved Multi-Image Instruction Tuning</a></div> <div class="author"> <em><b>Dongfu Jiang</b></em>, Xuan He, Huaye Zeng, Cong Wei, Max W.F. Ku, Qian Liu, and Wenhu Chen</div> <div class="periodical"> <em>Transactions on Machine Learning Research</em>, Dec 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2405.01483" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/badge/arXiv-b31b1b.svg?logo=arXiv"> </a> <a class="abstract z-depth-0" role="button"><img alt="Abstract" src="https://img.shields.io/badge/-Abs-FFCC99?style=flat"></a> <a class="bibtex z-depth-0" role="button"><img alt="Bibtex" src="https://img.shields.io/badge/-Bib-FFCC99?style=flat"></a> <a href="https://tiger-ai-lab.github.io/Mantis/" rel="external nofollow noopener" target="_blank"> <img alt="Website" src="https://img.shields.io/badge/-Website-FFCC99?style=flat"> </a> <a href="https://huggingface.co/collections/TIGER-Lab/mantis-6619b0834594c878cdb1d6e4" rel="external nofollow noopener" target="_blank"> <img alt="Huggingface Collections" src="https://img.shields.io/badge/-%F0%9F%A4%97%20Huggingface-red?style=flat"> </a> <a href="https://twitter.com/DongfuJiang/status/1786552974598078677" rel="external nofollow noopener" target="_blank"><img alt="X (formerly Twitter) URL" src="https://img.shields.io/badge/-Tweet-blue?style=flat&amp;logo=twitter"></a> <a href="https://github.com/TIGER-AI-Lab/Mantis" rel="external nofollow noopener" target="_blank"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/TIGER-AI-Lab/Mantis"></a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Large multimodal models (LMMs) have shown great results in single-image vision language tasks. However, their abilities to solve multi-image visual language tasks is yet to be improved. The existing LMMs like OpenFlamingo, Emu2, Idefics gain their multi-image ability through pre-training on hundreds of millions of noisy interleaved image-text data from the web, which is neither efficient nor effective. In this paper, we aim to build strong multi-image LMMs via instruction tuning with academic-level resources. Therefore, we meticulously construct Mantis-Instruct containing 721K multi-image instruction data to train a family of models Mantis. The instruction tuning empowers Mantis with different multi-image skills like co-reference, comparison, reasoning, and temporal understanding. We evaluate Mantis on five multi-image benchmarks and seven single-image benchmarks. Mantis-SigLIP can achieve SoTA results on all the multi-image benchmarks and beat the strongest multi-image baseline, Idefics2-8B by an average of 11 absolute points. Notably, Idefics2-8B was pre-trained on 140M interleaved multi-image data, which is 200x larger than Mantis-Instruct. We observe that Mantis performs equivalently well on the held-in and held-out benchmarks, which shows its generalization ability. Notably, we found that Mantis can even match the performance of GPT-4V on multi-image benchmarks. We further evaluate Mantis on single-image benchmarks and demonstrate that Mantis also maintains a strong single-image performance on par with CogVLM and Emu2. Our results show that multi-image abilities are not necessarily gained through massive pre-training, instead, it can be gained by the low-cost instruction tuning. Our work provides new perspectives on how to improve LMMs’ multi-image abilities.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Jiang2024MANTISIM</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MANTIS: Interleaved Multi-Image Instruction Tuning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jiang, Dongfu and He, Xuan and Zeng, Huaye and Wei, Cong and Ku, Max W.F. and Liu, Qian and Chen, Wenhu}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Transactions on Machine Learning Research}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2405.01483}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CV}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2405.01483}</span><span class="p">,</span>
  <span class="na">github</span> <span class="p">=</span> <span class="s">{TIGER-AI-Lab/Mantis}</span><span class="p">,</span>
  <span class="na">twitter</span> <span class="p">=</span> <span class="s">{https://twitter.com/DongfuJiang/status/1786552974598078677}</span><span class="p">,</span>
  <span class="na">huggingface</span> <span class="p">=</span> <span class="s">{https://huggingface.co/collections/TIGER-Lab/mantis-6619b0834594c878cdb1d6e4}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/mantis_preview-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/mantis_preview-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/mantis_preview-1400.webp"></source> <img src="/assets/img/publication_preview/mantis_preview.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="mantis_preview.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#dc143c"><a href="https://aclanthology.org" rel="external nofollow noopener" target="_blank">ACL 2024</a></abbr></div> <div id="Ku2023VIEScoreTE" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2312.14867" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank">VIEScore: Towards Explainable Metrics for Conditional Image Synthesis Evaluation</a></div> <div class="author"> Max Ku, <em><b>Dongfu Jiang</b></em>, Cong Wei, Xiang Yue, and Wenhu Chen</div> <div class="periodical"> <em>In Proceedings of ACL</em>, Aug 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2312.14867" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/badge/arXiv-b31b1b.svg?logo=arXiv"> </a> <a class="abstract z-depth-0" role="button"><img alt="Abstract" src="https://img.shields.io/badge/-Abs-FFCC99?style=flat"></a> <a class="bibtex z-depth-0" role="button"><img alt="Bibtex" src="https://img.shields.io/badge/-Bib-FFCC99?style=flat"></a> <a href="https://tiger-ai-lab.github.io/VIEScore/" rel="external nofollow noopener" target="_blank"> <img alt="Website" src="https://img.shields.io/badge/-Website-FFCC99?style=flat"> </a> <a href="https://twitter.com/DongfuJiang/status/1742043191732302076" rel="external nofollow noopener" target="_blank"><img alt="X (formerly Twitter) URL" src="https://img.shields.io/badge/-Tweet-blue?style=flat&amp;logo=twitter"></a> <a href="https://github.com/TIGER-AI-Lab/VIEScore" rel="external nofollow noopener" target="_blank"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/TIGER-AI-Lab/VIEScore"></a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>In the rapidly advancing field of conditional image generation research, challenges such as limited explainability lie in effectively evaluating the performance and capabilities of various models. This paper introduces VIEScore, a Visual Instruction-guided Explainable metric for evaluating any conditional image generation tasks. VIEScore leverages general knowledge from Multimodal Large Language Models (MLLMs) as the backbone and does not require training or fine-tuning. We evaluate VIEScore on seven prominent tasks in conditional image tasks and found: (1) VIEScore (GPT4-o) achieves a high Spearman correlation of 0.4 with human evaluations, while the human-to-human correlation is 0.45. (2) VIEScore (with open-source MLLM) is significantly weaker than GPT-4o and GPT-4v in evaluating synthetic images. (3) VIEScore achieves a correlation on par with human ratings in the generation tasks but struggles in editing tasks. With these results, we believe VIEScore shows its great potential to replace human judges in evaluating image synthesis tasks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Ku2023VIEScoreTE</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{VIEScore: Towards Explainable Metrics for Conditional Image Synthesis Evaluation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ku, Max and Jiang, Dongfu and Wei, Cong and Yue, Xiang and Chen, Wenhu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of ACL}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">aug</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{abs/2312.14867}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2024.acl-long.663}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2024.acl-long.663}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{12268--12290}</span><span class="p">,</span>
  <span class="na">github</span> <span class="p">=</span> <span class="s">{TIGER-AI-Lab/VIEScore}</span><span class="p">,</span>
  <span class="na">twitter</span> <span class="p">=</span> <span class="s">{https://twitter.com/DongfuJiang/status/1742043191732302076}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">false</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/viescore-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/viescore-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/viescore-1400.webp"></source> <img src="/assets/img/publication_preview/viescore.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="viescore.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#5b9bd5"><a href="https://jmlr.org/tmlr/" rel="external nofollow noopener" target="_blank">TMLR 2024</a></abbr></div> <div id="jiang2024tigerscore" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2310.00752" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank">TIGERScore: Towards Building Explainable Metric for All Text Generation Tasks</a></div> <div class="author"> <em><b>Dongfu Jiang</b></em><sup>*</sup>, Yishan Li<sup>*</sup>, Ge Zhang, Wenhao Huang, Bill Yuchen Lin, and Wenhu Chen</div> <div class="periodical"> <em>Transactions on Machine Learning Research (TMLR)</em>, May 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2310.00752" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/badge/arXiv-b31b1b.svg?logo=arXiv"> </a> <a class="abstract z-depth-0" role="button"><img alt="Abstract" src="https://img.shields.io/badge/-Abs-FFCC99?style=flat"></a> <a class="bibtex z-depth-0" role="button"><img alt="Bibtex" src="https://img.shields.io/badge/-Bib-FFCC99?style=flat"></a> <a href="https://docs.google.com/presentation/d/1-0uYzlR_-hUpgdfEksFWb65l0UouJ0W_/edit?usp=sharing&amp;ouid=112776303426789186559&amp;rtpof=true&amp;sd=true" class="z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><img alt="Website" src="https://img.shields.io/badge/-Slides-FFCC99?style=flat"></a> <a href="https://tiger-ai-lab.github.io/TIGERScore/" rel="external nofollow noopener" target="_blank"> <img alt="Website" src="https://img.shields.io/badge/-Website-FFCC99?style=flat"> </a> <a href="https://huggingface.co/collections/TIGER-Lab/tigerscore-657020bfae61260b6131f1ca" rel="external nofollow noopener" target="_blank"> <img alt="Huggingface Collections" src="https://img.shields.io/badge/-%F0%9F%A4%97%20Huggingface-red?style=flat"> </a> <a href="https://twitter.com/DongfuJiang/status/1735508082510168425" rel="external nofollow noopener" target="_blank"><img alt="X (formerly Twitter) URL" src="https://img.shields.io/badge/-Tweet-blue?style=flat&amp;logo=twitter"></a> <a href="https://github.com/TIGER-AI-Lab/TIGERScore" rel="external nofollow noopener" target="_blank"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/TIGER-AI-Lab/TIGERScore"></a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>We present TIGERScore, a \textbfTrained metric that follows \textbfInstruction \textbfGuidance to perform \textbfExplainable, and \textbfReference-free evaluation over a wide spectrum of text generation tasks. Different from other automatic evaluation methods that only provide arcane scores, TIGERScore is guided by natural language instruction to provide error analysis to pinpoint the mistakes in the generated text. Our metric is based on LLaMA-2, trained on our meticulously curated instruction-tuning dataset MetricInstruct which covers 6 text generation tasks and 23 text generation datasets. The dataset consists of 42K quadruple in the form of (instruction, input, system output error analysis). We collected the ‘system outputs’ through from a large variety of models to cover different types of errors. To quantitatively assess our metric, we evaluate its correlation with human ratings on 5 held-in datasets, 2 held-out datasets and show that \metricname can achieve the open-source SoTA correlation with human ratings across these datasets and almost approaches GPT-4 evaluator. As a reference-free metric, its correlation can even surpass the best existing reference-based metrics. To further qualitatively assess the rationale generated by our metric, we conduct human evaluation on the generated explanations and found that the explanations are 70.8% accurate. Through these experimental results, we believe \metricname demonstrates the possibility of building universal explainable metrics to evaluate any text generation task.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">jiang2024tigerscore</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{TIGERS}core: Towards Building Explainable Metric for All Text Generation Tasks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jiang, Dongfu and Li, Yishan and Zhang, Ge and Huang, Wenhao and Lin, Bill Yuchen and Chen, Wenhu}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Transactions on Machine Learning Research (TMLR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">github</span> <span class="p">=</span> <span class="s">{TIGER-AI-Lab/TIGERScore}</span><span class="p">,</span>
  <span class="na">twitter</span> <span class="p">=</span> <span class="s">{https://twitter.com/DongfuJiang/status/1735508082510168425}</span><span class="p">,</span>
  <span class="na">huggingface</span> <span class="p">=</span> <span class="s">{https://huggingface.co/collections/TIGER-Lab/tigerscore-657020bfae61260b6131f1ca}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2835-8856}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=EE1CBKC0SZ}</span><span class="p">,</span>
  <span class="na">num_co_first_author</span> <span class="p">=</span> <span class="s">{2}</span>
<span class="p">}</span></code></pre></figure> </div> </div> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/tigerscore_preview-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/tigerscore_preview-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/tigerscore_preview-1400.webp"></source> <img src="/assets/img/publication_preview/tigerscore_preview.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="tigerscore_preview.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="https://cvpr.thecvf.com" rel="external nofollow noopener" target="_blank">CVPR 2024</a></abbr></div> <div id="Yue2023MMMUAM" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2311.16502" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank">MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI</a></div> <div class="author"> Xiang Yue<sup>*</sup>, Yuansheng Ni<sup>*</sup>, Kai Zhang<sup>*</sup>, Tianyu Zheng<sup>*</sup>, Ruoqi Liu, Ge Zhang, Samuel Stevens, <em><b>Dongfu Jiang</b></em>, and <span class="more-authors" title="click to view 14 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '14 more authors' ? 'Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen' : '14 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">14 more authors</span> </div> <div class="periodical"> <em>In Proceedings of CVPR <span style="color: red; font-weight: bold;">oral</span></em>, Jun 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2311.16502" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/badge/arXiv-b31b1b.svg?logo=arXiv"> </a> <a class="abstract z-depth-0" role="button"><img alt="Abstract" src="https://img.shields.io/badge/-Abs-FFCC99?style=flat"></a> <a class="bibtex z-depth-0" role="button"><img alt="Bibtex" src="https://img.shields.io/badge/-Bib-FFCC99?style=flat"></a> <a href="https://mmmu-benchmark.github.io" rel="external nofollow noopener" target="_blank"> <img alt="Website" src="https://img.shields.io/badge/-Website-FFCC99?style=flat"> </a> <a href="https://huggingface.co/datasets/MMMU/MMMU" rel="external nofollow noopener" target="_blank"> <img alt="Huggingface Collections" src="https://img.shields.io/badge/-%F0%9F%A4%97%20Huggingface-red?style=flat"> </a> <a href="https://twitter.com/xiangyue96/status/1729698316554801358" rel="external nofollow noopener" target="_blank"><img alt="X (formerly Twitter) URL" src="https://img.shields.io/badge/-Tweet-blue?style=flat&amp;logo=twitter"></a> <a href="https://github.com/MMMU-Benchmark/MMMU" rel="external nofollow noopener" target="_blank"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/MMMU-Benchmark/MMMU"></a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>We introduce MMMU: a new benchmark designed to evaluate multimodal models on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning. MMMU includes 11.5K meticulously collected multimodal questions from college exams quizzes and textbooks covering six core disciplines: Art &amp; Design Business Science Health &amp; Medicine Humanities &amp; Social Science and Tech &amp; Engineering. These questions span 30 subjects and 183 subfields comprising 30 highly heterogeneous image types such as charts diagrams maps tables music sheets and chemical structures. Unlike existing benchmarks MMMU focuses on advanced perception and reasoning with domain-specific knowledge challenging models to perform tasks akin to those faced by experts. The evaluation of 28 open-source LMMs as well as the proprietary GPT-4V(ision) and Gemini highlights the substantial challenges posed by MMMU. Even the advanced GPT-4V and Gemini Ultra only achieve accuracies of 56% and 59% respectively indicating significant room for improvement. We believe MMMU will stimulate the community to build next-generation multimodal foundation models towards expert artificial general intelligence.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Yue2023MMMUAM</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yue, Xiang and Ni, Yuansheng and Zhang, Kai and Zheng, Tianyu and Liu, Ruoqi and Zhang, Ge and Stevens, Samuel and Jiang, Dongfu and Ren, Weiming and Sun, Yuxuan and Wei, Cong and Yu, Botao and Yuan, Ruibin and Sun, Renliang and Yin, Ming and Zheng, Boyuan and Yang, Zhenzhu and Liu, Yibo and Huang, Wenhao and Sun, Huan and Su, Yu and Chen, Wenhu}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Seattle, US}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of CVPR &lt;span style="color: red; font-weight: bold;"&gt;oral&lt;/span&gt;}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2311.16502}</span><span class="p">,</span>
  <span class="na">github</span> <span class="p">=</span> <span class="s">{MMMU-Benchmark/MMMU}</span><span class="p">,</span>
  <span class="na">twitter</span> <span class="p">=</span> <span class="s">{https://twitter.com/xiangyue96/status/1729698316554801358}</span><span class="p">,</span>
  <span class="na">huggingface</span> <span class="p">=</span> <span class="s">{https://huggingface.co/datasets/MMMU/MMMU}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">num_co_first_author</span> <span class="p">=</span> <span class="s">{4}</span>
<span class="p">}</span></code></pre></figure> </div> </div> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/mmmu_preview-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/mmmu_preview-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/mmmu_preview-1400.webp"></source> <img src="/assets/img/publication_preview/mmmu_preview.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="mmmu_preview.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#dc143c"><a href="https://aclanthology.org" rel="external nofollow noopener" target="_blank">ACL 2023</a></abbr></div> <div id="jiang-etal-2023-llm" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2306.02561" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank">LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion</a></div> <div class="author"> <em><b>Dongfu Jiang</b></em>, Xiang Ren, and Bill Yuchen Lin</div> <div class="periodical"> <em>In Proceedings of ACL</em>, Jul 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2306.02561" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/badge/arXiv-b31b1b.svg?logo=arXiv"> </a> <a class="abstract z-depth-0" role="button"><img alt="Abstract" src="https://img.shields.io/badge/-Abs-FFCC99?style=flat"></a> <a class="bibtex z-depth-0" role="button"><img alt="Bibtex" src="https://img.shields.io/badge/-Bib-FFCC99?style=flat"></a> <a href="https://docs.google.com/presentation/d/1UvTYEjIjpPGjrvBxtEmAtC06N-7xFFY9/edit?usp=sharing&amp;ouid=112776303426789186559&amp;rtpof=true&amp;sd=true" class="z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><img alt="Website" src="https://img.shields.io/badge/-Slides-FFCC99?style=flat"></a> <a href="https://yuchenlin.xyz/LLM-Blender/" rel="external nofollow noopener" target="_blank"> <img alt="Website" src="https://img.shields.io/badge/-Website-FFCC99?style=flat"> </a> <a href="https://blog.allenai.org/llm-blender-a-simple-ensemble-learning-framework-for-llms-9e4bc57af23e" rel="external nofollow noopener" target="_blank"> <img alt="Blog" src="https://img.shields.io/badge/-Blog-FFCC99?style=flat"> </a> <a href="https://huggingface.co/llm-blender" rel="external nofollow noopener" target="_blank"> <img alt="Huggingface Collections" src="https://img.shields.io/badge/-%F0%9F%A4%97%20Huggingface-red?style=flat"> </a> <a href="https://twitter.com/billyuchenlin/status/1668666357058277377" rel="external nofollow noopener" target="_blank"><img alt="X (formerly Twitter) URL" src="https://img.shields.io/badge/-Tweet-blue?style=flat&amp;logo=twitter"></a> <a href="https://github.com/yuchenlin/LLM-Blender" rel="external nofollow noopener" target="_blank"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/yuchenlin/LLM-Blender"></a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>We present LLM-Blender, an ensembling framework designed to attain consistently superior performance by leveraging the diverse strengths of multiple open-source large language models (LLMs). Our framework consists of two modules: PairRanker and GenFuser, addressing the observation that optimal LLMs for different examples can significantly vary. PairRanker employs a specialized pairwise comparison method to distinguish subtle differences between candidate outputs. It jointly encodes the input text and a pair of candidates, using cross-attention encoders to determine the superior one. Our results demonstrate that PairRanker exhibits the highest correlation with ChatGPT-based ranking. Then, GenFuser aims to merge the top-ranked candidates, generating an improved output by capitalizing on their strengths and mitigating their weaknesses. To facilitate large-scale evaluation, we introduce a benchmark dataset, MixInstruct, which is a mixture of multiple instruction datasets featuring oracle pairwise comparisons. Our LLM-Blender significantly outperform individual LLMs and baseline methods across various metrics, establishing a substantial performance gap.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">jiang-etal-2023-llm</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{LLM}-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jiang, Dongfu and Ren, Xiang and Lin, Bill Yuchen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of ACL}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Toronto, Canada}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2023.acl-long.792}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2023.acl-long.792}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{14165--14178}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">github</span> <span class="p">=</span> <span class="s">{yuchenlin/LLM-Blender}</span><span class="p">,</span>
  <span class="na">twitter</span> <span class="p">=</span> <span class="s">{https://twitter.com/billyuchenlin/status/1668666357058277377}</span><span class="p">,</span>
  <span class="na">huggingface</span> <span class="p">=</span> <span class="s">{https://huggingface.co/llm-blender}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/llm_blender_preview-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/llm_blender_preview-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/llm_blender_preview-1400.webp"></source> <img src="/assets/img/publication_preview/llm_blender_preview.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="llm_blender_preview.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Dongfu Jiang (姜东甫). Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: February 13, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script>var wechatModal=document.getElementById("WeChatMod"),wechatBtn=document.getElementById("WeChatBtn");wechatBtn.onclick=function(){wechatModal.style.display="block"},window.onclick=function(t){t.target==wechatModal&&(wechatModal.style.display="none")};</script> </body> </html>